{"cells":[{"cell_type":"code","source":["# Create library using Mavern coordinate\n# Enter \"databricks:tensorframes:0.2.8-s_2.10\" and search for this library\n# Attach it onto your current working cluster\n# Reference: https://github.com/databricks/tensorframes/wiki/TensorFrames-user-guide\n\nimport tensorframes as tfs\nimport tensorflow as tf\nfrom pyspark.sql import Row\nfrom pyspark.sql.functions import *"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["data = [Row(x=float(x)) for x in range(10)]\ndf = sqlContext.createDataFrame(data)\ntfs.print_schema(df)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["data"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["with tf.Graph().as_default() as g:\n  \n    x = tfs.block(df, \"x\")\n    # The output that adds 3 to x\n    z = tf.add(x, 3, name=\"z\")\n    # The resulting dataframe\n    df2 = tfs.map_blocks(z, df)\n    \ndf2.collect()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Build a DataFrame of vectors\ndata_2 = [Row(y=[float(y), float(-y)]) for y in range(10)]\ndf = sqlContext.createDataFrame(data_2)\n# Because the dataframe contains vectors, we need to analyze it first to find the\n# dimensions of the vectors.\ndf2 = tfs.analyze(df)\n\n# The information gathered by TF can be printed to check the content:\ntfs.print_schema(df2)\n# TF has inferred that y contains vectors of size 2\n# root\n#  |-- y: array (nullable = false) DoubleType[?,2]\n\n# Let's use the analyzed dataframe to compute the sum and the elementwise minimum \n# of all the vectors:\n# First, let's make a copy of the 'y' column. This will be very cheap in Spark 2.0+\ndf3 = df2.select(df2.y, df2.y.alias(\"z\"))\nwith tf.Graph().as_default() as g:\n    # The placeholders. Note the special name that end with '_input':\n    y_input = tfs.block(df3, 'y', tf_name=\"y_input\")\n    z_input = tfs.block(df3, 'z', tf_name=\"z_input\")\n    y = tf.reduce_sum(y_input, [0], name='y')\n    z = tf.reduce_min(z_input, [0], name='z')\n    # The resulting dataframe\n    (data_sum, data_min) = tfs.reduce_blocks([y, z], df3)\n\n# The final results are numpy arrays:\nprint data_sum\n# [45.0, -45.0]\nprint data_min\n# [0.0, -9.0]"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["data_3 = [Row(x=[float(x), float(2 * x)],\n\t \t\tkey=str(x % 2),\n\t \t\tz = float(x+1)) for x in range(1, 6)]\ndf4 = sqlContext.createDataFrame(data_3)\ntfs.print_schema(df4)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# The analyze() method can be used to gather more information about the shape. It is relatively expensive to run.\ndf5 = tfs.analyze(df4)\ntfs.print_schema(df5)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["data_4 = [Row(x=float(x)) for x in range(5)]\ndf6 = sqlContext.createDataFrame(data_4)\nwith tf.Graph().as_default() as g:\n    # The placeholder that corresponds to column 'x'\n    x = tf.placeholder(tf.double, shape=[None], name=\"x\")\n    # The output that adds 3 to x\n    z = tf.add(x, 3, name='z')\n    # The resulting dataframe\n    df7 = tfs.map_blocks(z, df6)\n\ndf7.show()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# Following code computes the sum of a column by repeatedly summing pairs of columns together\ndata_5 = [Row(x=float(x)) for x in range(5)]\ndf8 = sqlContext.createDataFrame(data_5)\n\nwith tf.Graph().as_default() as g:\n    # The placeholders that correspond to column 'x'.\n    # Note the convention of calling them with '_1' and '_2'\n    x_1 = tf.placeholder(tf.double, shape=[], name=\"x_1\")\n    x_2 = tf.placeholder(tf.double, shape=[], name=\"x_2\")\n    # We sum the two inputs.\n    x = tf.add(x_1, x_2, name='x')\n    # The resulting number\n    res = tfs.reduce_rows(x, df8)\nprint res"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# computes the harmonic mean of same values, aggregated by a key\n\ndata = [Row(x=[float(x), float(2 * x)], key=str(x % 2)) for x in range(1, 6)]\n# The rest of the example works without modification if we replace the data with scalars:\n# data = [Row(x=float(x), key=str(x % 2)) for x in range(1, 6)]\n# The analysis is not required if x is a real\ndf = tfs.analyze(sqlContext.createDataFrame(data))\ncol_name = \"x\"\ncol_key = \"key\"\n\nwith tf.Graph().as_default() as g:\n    x = tfs.block(df, col_name)\n    invs = tf.divide(tf.to_double(1.0), tf.to_double(x), name=\"invs\")\n    df2 = tfs.map_blocks([invs, tf.ones_like(invs, name=\"count\")], df)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["gb = df2.select(col_key, \"invs\", \"count\").groupBy(col_key)\nwith tf.Graph().as_default() as g:\n\t# Look at the documentation of tfs.aggregate for the naming conventions of the placeholders.\n    x_input = tfs.block(df2, \"invs\", tf_name=\"invs_input\")\n    count_input = tfs.block(df2, \"invs\", tf_name=\"count_input\")\n    x = tf.reduce_sum(x_input, [0], name='invs')\n    count = tf.reduce_sum(count_input, [0], name='count')\n    df3 = tfs.aggregate([x, count], gb)\n    \nwith tf.Graph().as_default() as g:\n    invs = tfs.block(df2, \"invs\")\n    count = tfs.block(df2, \"count\")\n    geom_mean = tf.div(tf.to_double(count), invs,  name = \"harmonic_mean\")\n    df4 = tfs.map_blocks(geom_mean, df3).select(\"key\", \"harmonic_mean\")\n\ndf4.show()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":12}],"metadata":{"name":"Tensorframe_test","notebookId":534030395876828},"nbformat":4,"nbformat_minor":0}
